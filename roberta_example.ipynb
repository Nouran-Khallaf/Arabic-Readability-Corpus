{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nouran-Khallaf/Arabic-Readability-Corpus/blob/main/roberta_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sitting-robin",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sitting-robin",
        "outputId": "0381f1b9-f5bd-459e-aa03-10cd549268d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.20.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dirty-steal",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dirty-steal",
        "outputId": "328e3aef-6c9c-4c12-d162-1f18052e983b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "#     !nvidia-smi\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NA5eAVi19h-w"
      },
      "id": "NA5eAVi19h-w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "democratic-arbor",
      "metadata": {
        "id": "democratic-arbor"
      },
      "source": [
        "## Creating Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "legendary-flexibility",
      "metadata": {
        "id": "legendary-flexibility"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import csv\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_score , recall_score\n",
        "\n",
        "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, BertTokenizer\n",
        "from transformers.data.processors import SingleSentenceClassificationProcessor\n",
        "from transformers import Trainer , TrainingArguments\n",
        "from transformers.trainer_utils import EvaluationStrategy\n",
        "from transformers.data.processors.utils import InputFeatures\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.utils import resample\n",
        "import logging\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "muslim-departure",
      "metadata": {
        "id": "muslim-departure"
      },
      "outputs": [],
      "source": [
        "class Dataset:\n",
        "    def __init__(\n",
        "        self,\n",
        "        name,\n",
        "        train,\n",
        "        test,\n",
        "        label_list,\n",
        "    ):\n",
        "        self.name = name\n",
        "        self.train = train\n",
        "        self.test = test\n",
        "        self.label_list = label_list\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://www.inf.ed.ac.uk/teaching/courses/tts/labs/lab7/tweetsclassification.zip\n",
        "!unzip tweetsclassification.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmOKZQsM5QqV",
        "outputId": "846725a1-1d5a-428d-fe7c-b040276b3525"
      },
      "id": "RmOKZQsM5QqV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-17 14:03:31--  http://www.inf.ed.ac.uk/teaching/courses/tts/labs/lab7/tweetsclassification.zip\n",
            "Resolving www.inf.ed.ac.uk (www.inf.ed.ac.uk)... 129.215.33.176\n",
            "Connecting to www.inf.ed.ac.uk (www.inf.ed.ac.uk)|129.215.33.176|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://www.inf.ed.ac.uk/teaching/courses/tts/labs/lab7/tweetsclassification.zip [following]\n",
            "--2022-06-17 14:03:31--  https://www.inf.ed.ac.uk/teaching/courses/tts/labs/lab7/tweetsclassification.zip\n",
            "Connecting to www.inf.ed.ac.uk (www.inf.ed.ac.uk)|129.215.33.176|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 209770 (205K) [application/zip]\n",
            "Saving to: ‘tweetsclassification.zip.1’\n",
            "\n",
            "tweetsclassificatio 100%[===================>] 204.85K   569KB/s    in 0.4s    \n",
            "\n",
            "2022-06-17 14:03:32 (569 KB/s) - ‘tweetsclassification.zip.1’ saved [209770/209770]\n",
            "\n",
            "Archive:  tweetsclassification.zip\n",
            "replace Tweets.14cat.train? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: Tweets.14cat.train      \n",
            "replace Tweets.14cat.test? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: Tweets.14cat.test       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "integrated-colleague",
      "metadata": {
        "id": "integrated-colleague"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('Tweets.14cat.train',encoding='latin-1', sep='\\t',header=None,quoting=csv.QUOTE_NONE)\n",
        "test = pd.read_csv('Tweets.14cat.test',encoding='latin-1', sep='\\t', header=None,quoting=csv.QUOTE_NONE)\n",
        "\n",
        "# Assigning names to our columns\n",
        "train.columns=['id','tweet','category']\n",
        "test.columns=['id','tweet','category']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "technical-vacuum",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "technical-vacuum",
        "outputId": "92acdf55-5acf-4dd9-9eef-2d8221f8250d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Pets & Animals', 'Comedy', 'Autos & Vehicles', 'Science & Technology', 'News & Politics', 'Gaming', 'Nonprofits & Activism', 'Music', 'Film & Animation', 'Education', 'Travel & Events', 'Entertainment', 'Sports', 'Howto & Style']\n",
            "Gaming                   220\n",
            "Autos & Vehicles         210\n",
            "Howto & Style            207\n",
            "Sports                   203\n",
            "Travel & Events          196\n",
            "Science & Technology     189\n",
            "Film & Animation         178\n",
            "Pets & Animals           177\n",
            "News & Politics          168\n",
            "Music                    160\n",
            "Entertainment            159\n",
            "Comedy                   153\n",
            "Education                142\n",
            "Nonprofits & Activism    141\n",
            "Name: category, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "DATA_COLUMN = 'tweet'\n",
        "LABEL_COLUMN = 'category'\n",
        "\n",
        "labels= list(train[LABEL_COLUMN].unique())\n",
        "print(labels)\n",
        "print(train[LABEL_COLUMN].value_counts())\n",
        "\n",
        "dataset = Dataset(\"example\", train, test, labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "common-flight",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "common-flight",
        "outputId": "dc9d5e55-95ac-4215-b654-a0ca2124d68c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                      id                                              tweet  \\\n",
              "0      45029314109075046  Furniture for - so cute! gotta show my #grandd...   \n",
              "1      45033090867215155                         \"#Sunday aww\"\": Mr Peebles   \n",
              "2      45036625162627481  CATS ART http://t.co/cJre1jn2Bl #creative #fel...   \n",
              "3      45086603513077350  RT @Masala_chaai: Keep Calm & Hug your Dog ! #...   \n",
              "4      45138968053405286  RT @TheSoulfulEMU: RETWEET if you love your do...   \n",
              "...                  ...                                                ...   \n",
              "2498  551069446257655808  Series of Car Window Breakages Under Investiga...   \n",
              "2499  551156371031199744   night Maltese club offer Evans contract: Conv...   \n",
              "2500  551166096598781952  All of today's news headlines in one place. ht...   \n",
              "2501  551617864805781504  ISBPL: Affordable housing scheme for EPFO subs...   \n",
              "2502  551831514485231616   Police hunt hotel murder suspect: Police have...   \n",
              "\n",
              "             category  \n",
              "0      Pets & Animals  \n",
              "1      Pets & Animals  \n",
              "2      Pets & Animals  \n",
              "3      Pets & Animals  \n",
              "4      Pets & Animals  \n",
              "...               ...  \n",
              "2498  News & Politics  \n",
              "2499  News & Politics  \n",
              "2500  News & Politics  \n",
              "2501  News & Politics  \n",
              "2502  News & Politics  \n",
              "\n",
              "[2503 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-15681776-6d32-4684-bf92-7a1b073b743c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>45029314109075046</td>\n",
              "      <td>Furniture for - so cute! gotta show my #grandd...</td>\n",
              "      <td>Pets &amp; Animals</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>45033090867215155</td>\n",
              "      <td>\"#Sunday aww\"\": Mr Peebles</td>\n",
              "      <td>Pets &amp; Animals</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>45036625162627481</td>\n",
              "      <td>CATS ART http://t.co/cJre1jn2Bl #creative #fel...</td>\n",
              "      <td>Pets &amp; Animals</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>45086603513077350</td>\n",
              "      <td>RT @Masala_chaai: Keep Calm &amp; Hug your Dog ! #...</td>\n",
              "      <td>Pets &amp; Animals</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>45138968053405286</td>\n",
              "      <td>RT @TheSoulfulEMU: RETWEET if you love your do...</td>\n",
              "      <td>Pets &amp; Animals</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2498</th>\n",
              "      <td>551069446257655808</td>\n",
              "      <td>Series of Car Window Breakages Under Investiga...</td>\n",
              "      <td>News &amp; Politics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2499</th>\n",
              "      <td>551156371031199744</td>\n",
              "      <td>night Maltese club offer Evans contract: Conv...</td>\n",
              "      <td>News &amp; Politics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2500</th>\n",
              "      <td>551166096598781952</td>\n",
              "      <td>All of today's news headlines in one place. ht...</td>\n",
              "      <td>News &amp; Politics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2501</th>\n",
              "      <td>551617864805781504</td>\n",
              "      <td>ISBPL: Affordable housing scheme for EPFO subs...</td>\n",
              "      <td>News &amp; Politics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2502</th>\n",
              "      <td>551831514485231616</td>\n",
              "      <td>Police hunt hotel murder suspect: Police have...</td>\n",
              "      <td>News &amp; Politics</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2503 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-15681776-6d32-4684-bf92-7a1b073b743c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-15681776-6d32-4684-bf92-7a1b073b743c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-15681776-6d32-4684-bf92-7a1b073b743c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "general-interim",
      "metadata": {
        "id": "general-interim"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig(level=logging.WARNING)\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sapphire-tomorrow",
      "metadata": {
        "id": "sapphire-tomorrow"
      },
      "outputs": [],
      "source": [
        "# preprocessing\n",
        "import re\n",
        "import string\n",
        "def preprocess(sent):\n",
        "    x=\"\".join([i for i in sent if i not in string.punctuation])\n",
        "    return x.lower()\n",
        "dataset.train[DATA_COLUMN] = dataset.train[DATA_COLUMN].apply(lambda x: preprocess(x))\n",
        "dataset.test[DATA_COLUMN] = dataset.test[DATA_COLUMN].apply(lambda x: preprocess(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sustainable-drunk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sustainable-drunk",
        "outputId": "bb378df9-b624-4435-d42c-ee366dc86911"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     furniture for  so cute gotta show my granddog ...\n",
              "1                                 sunday aww mr peebles\n",
              "2     cats art httptcocjre1jn2bl creative feline art...\n",
              "3     rt masalachaai keep calm  hug your dog  petlov...\n",
              "4     rt thesoulfulemu retweet if you love your dog ...\n",
              "5     missing cat atlantic gardens httptcoe2mu2yiv6h...\n",
              "6     rt doggystylin firsttime customers receive a 1...\n",
              "7                                      rt petsweekly so\n",
              "8     the first movie with a drakonia included in th...\n",
              "9     bmw plans to invest up to us1 billion a108 bil...\n",
              "10    so to oxygen for being a total pimp and bondin...\n",
              "11                                                   us\n",
              "12    your twitter conversations fall into one of th...\n",
              "13    capture in hd capture everything order now htt...\n",
              "14    hockenheim veterama 2014 parts teile marked ve...\n",
              "15    african women lag men in activism httptcoaii9x...\n",
              "16    sytner opens lamborghini dealership in leicest...\n",
              "17    27off deal 1799 looking into you a tribute to ...\n",
              "18    really thinking about getting dayz who has it ...\n",
              "19    olympus stylus sh1 compact camera with 5axis o...\n",
              "Name: tweet, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "dataset.train[DATA_COLUMN][:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "diagnostic-logging",
      "metadata": {
        "id": "diagnostic-logging"
      },
      "outputs": [],
      "source": [
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, text, target, model_name, max_len, label_map):\n",
        "      super(BERTDataset).__init__()\n",
        "      self.text = text\n",
        "      self.target = target\n",
        "      self.tokenizer_name = model_name\n",
        "      self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "      self.max_len = max_len\n",
        "      self.label_map = label_map\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.text)\n",
        "\n",
        "    def __getitem__(self,item):\n",
        "      text = str(self.text[item])\n",
        "      text = \" \".join(text.split())\n",
        "\n",
        "\n",
        "\n",
        "      input_ids = self.tokenizer.encode(\n",
        "          text,\n",
        "          add_special_tokens=True,\n",
        "          max_length=self.max_len,\n",
        "          truncation='longest_first'\n",
        "      )\n",
        "\n",
        "      attention_mask = [1] * len(input_ids)\n",
        "\n",
        "      # Zero-pad up to the sequence length.\n",
        "      padding_length = self.max_len - len(input_ids)\n",
        "      input_ids = input_ids + ([self.tokenizer.pad_token_id] * padding_length)\n",
        "      attention_mask = attention_mask + ([0] * padding_length)\n",
        "\n",
        "      return InputFeatures(input_ids=input_ids, attention_mask=attention_mask, label=self.label_map[self.target[item]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "comic-fraction",
      "metadata": {
        "id": "comic-fraction"
      },
      "outputs": [],
      "source": [
        "# some specs of our model\n",
        "num_of_epochs=8\n",
        "model_name = 'roberta-large'\n",
        "task_name = 'classification'\n",
        "max_len = 128 #128#256\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "straight-butter",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "straight-butter",
        "outputId": "bada5427-8129-443c-f70d-9e33d7eb992f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Pets & Animals': 0, 'Comedy': 1, 'Autos & Vehicles': 2, 'Science & Technology': 3, 'News & Politics': 4, 'Gaming': 5, 'Nonprofits & Activism': 6, 'Music': 7, 'Film & Animation': 8, 'Education': 9, 'Travel & Events': 10, 'Entertainment': 11, 'Sports': 12, 'Howto & Style': 13}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/roberta-large/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "loading file https://huggingface.co/roberta-large/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "loading file https://huggingface.co/roberta-large/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "loading file https://huggingface.co/roberta-large/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
            "loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/roberta-large/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "loading file https://huggingface.co/roberta-large/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "loading file https://huggingface.co/roberta-large/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "loading file https://huggingface.co/roberta-large/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
            "loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "label_map = { v:index for index, v in enumerate(dataset.label_list) }\n",
        "print(label_map)\n",
        "train_dataset = BERTDataset(dataset.train[DATA_COLUMN].to_list(),dataset.train[LABEL_COLUMN].to_list(),model_name,max_len,label_map)\n",
        "test_dataset = BERTDataset(dataset.test[DATA_COLUMN].to_list(),dataset.test[LABEL_COLUMN].to_list(),model_name,max_len,label_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vocational-brown",
      "metadata": {
        "id": "vocational-brown"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "express-stuart",
      "metadata": {
        "id": "express-stuart"
      },
      "outputs": [],
      "source": [
        "def model_init():\n",
        "    return AutoModelForSequenceClassification.from_pretrained(model_name, return_dict=True, num_labels=len(label_map))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "empirical-dictionary",
      "metadata": {
        "id": "empirical-dictionary"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(p): #p should be of type EvalPrediction\n",
        "  preds = np.argmax(p.predictions, axis=1)\n",
        "  assert len(preds) == len(p.label_ids)\n",
        "  macro_f1 = f1_score(p.label_ids,preds,average='macro')\n",
        "  macro_precision = precision_score(p.label_ids,preds,average='macro')\n",
        "  macro_recall = recall_score(p.label_ids,preds,average='macro')\n",
        "  acc = accuracy_score(p.label_ids,preds)\n",
        "  return {\n",
        "\n",
        "      'macro_f1' : macro_f1,\n",
        "      'macro_precision': macro_precision,\n",
        "      'macro_recall': macro_recall,\n",
        "      'accuracy': acc\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "extended-armstrong",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "extended-armstrong",
        "outputId": "d00481f7-d405-474b-a70d-b26cf3fc1219"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "156\n",
            "1248\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# training arguements\n",
        "training_args = TrainingArguments(\"./train\")\n",
        "training_args.evaluate_during_training = True\n",
        "training_args.adam_epsilon = 1e-8\n",
        "training_args.learning_rate = 5e-5#2e-4\n",
        "training_args.fp16 = True\n",
        "training_args.per_device_train_batch_size = 8\n",
        "training_args.per_device_eval_batch_size = 8\n",
        "training_args.gradient_accumulation_steps = 2\n",
        "training_args.num_train_epochs= num_of_epochs\n",
        "\n",
        "\n",
        "steps_per_epoch = (len(dataset.train)// (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps))\n",
        "total_steps = steps_per_epoch * training_args.num_train_epochs\n",
        "print(steps_per_epoch)\n",
        "print(total_steps)\n",
        "#Warmup_ratio\n",
        "warmup_ratio = 0.1\n",
        "training_args.warmup_steps = total_steps*warmup_ratio # or you can set the warmup steps directly\n",
        "\n",
        "training_args._n_gpu = 1\n",
        "training_args.evaluation_strategy = EvaluationStrategy.EPOCH\n",
        "# training_args.logging_steps = 200\n",
        "training_args.save_steps = 100000 #don't want to save any model, there is probably a better way to do this :)\n",
        "training_args.seed = 42\n",
        "training_args.disable_tqdm = False\n",
        "training_args.lr_scheduler_type = 'cosine'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "under-spare",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "under-spare",
        "outputId": "cc258750-100e-477f-91a6-d943c1400993"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\",\n",
            "    \"9\": \"LABEL_9\",\n",
            "    \"10\": \"LABEL_10\",\n",
            "    \"11\": \"LABEL_11\",\n",
            "    \"12\": \"LABEL_12\",\n",
            "    \"13\": \"LABEL_13\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_10\": 10,\n",
            "    \"LABEL_11\": 11,\n",
            "    \"LABEL_12\": 12,\n",
            "    \"LABEL_13\": 13,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8,\n",
            "    \"LABEL_9\": 9\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\n",
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using cuda_amp half precision backend\n"
          ]
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    model_init=model_init,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "champion-pollution",
      "metadata": {
        "id": "champion-pollution"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "distant-debate",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "distant-debate",
        "outputId": "04468a8c-4c1f-4108-f0ae-7b9c25d23808"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "roberta-large\n"
          ]
        }
      ],
      "source": [
        "print(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "coastal-folder",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "coastal-folder",
        "outputId": "fdc9f050-c134-4d33-c633-638d051bc2fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\",\n",
            "    \"9\": \"LABEL_9\",\n",
            "    \"10\": \"LABEL_10\",\n",
            "    \"11\": \"LABEL_11\",\n",
            "    \"12\": \"LABEL_12\",\n",
            "    \"13\": \"LABEL_13\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_10\": 10,\n",
            "    \"LABEL_11\": 11,\n",
            "    \"LABEL_12\": 12,\n",
            "    \"LABEL_13\": 13,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8,\n",
            "    \"LABEL_9\": 9\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
            "Wall time: 10 µs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 2503\n",
            "  Num Epochs = 8\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 1248\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1248' max='1248' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1248/1248 10:37, Epoch 7/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1248, training_loss=0.6547261659915631, metrics={'train_runtime': 638.0086, 'train_samples_per_second': 31.385, 'train_steps_per_second': 1.956, 'total_flos': 4663806613522944.0, 'train_loss': 0.6547261659915631, 'epoch': 8.0})"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "%time\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sudden-validation",
      "metadata": {
        "id": "sudden-validation"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "quarterly-apartment",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "quarterly-apartment",
        "outputId": "7663ff0d-4e91-46c3-85ef-ecf6cd6388c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 625\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [79/79 00:04]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'epoch': 8.0,\n",
              " 'eval_accuracy': 0.8,\n",
              " 'eval_loss': 1.142515778541565,\n",
              " 'eval_macro_f1': 0.7952967545814499,\n",
              " 'eval_macro_precision': 0.7970369595477268,\n",
              " 'eval_macro_recall': 0.7965738274745929,\n",
              " 'eval_runtime': 4.9624,\n",
              " 'eval_samples_per_second': 125.946,\n",
              " 'eval_steps_per_second': 15.92}"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "double-savannah",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "double-savannah",
        "outputId": "e9ffe516-e9a9-4d07-e234-e982cdb33726"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples = 625\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='158' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [79/79 00:09]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "result=trainer.predict(test_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "thousand-casino",
      "metadata": {
        "id": "thousand-casino"
      },
      "outputs": [],
      "source": [
        "pred = np.argmax(result.predictions, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "incoming-hearts",
      "metadata": {
        "id": "incoming-hearts"
      },
      "outputs": [],
      "source": [
        "mapper={y:x for x,y in label_map.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "stable-radical",
      "metadata": {
        "id": "stable-radical"
      },
      "outputs": [],
      "source": [
        "predictions=[mapper[p]for p in pred]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "operational-training",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "operational-training",
        "outputId": "5ebc6dd8-53d0-410a-86b4-2811724e2486"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                       precision    recall  f1-score   support\n",
            "\n",
            "     Autos & Vehicles       0.98      0.88      0.93        51\n",
            "               Comedy       0.75      0.79      0.77        38\n",
            "            Education       0.72      0.83      0.77        41\n",
            "        Entertainment       0.76      0.76      0.76        49\n",
            "     Film & Animation       0.67      0.63      0.65        46\n",
            "               Gaming       0.88      0.88      0.88        50\n",
            "        Howto & Style       0.78      0.90      0.84        40\n",
            "                Music       0.79      0.78      0.78        40\n",
            "      News & Politics       0.74      0.68      0.70        37\n",
            "Nonprofits & Activism       0.85      0.74      0.79        38\n",
            "       Pets & Animals       0.84      0.96      0.90        45\n",
            " Science & Technology       0.71      0.70      0.71        43\n",
            "               Sports       0.80      0.81      0.80        53\n",
            "      Travel & Events       0.88      0.83      0.86        54\n",
            "\n",
            "             accuracy                           0.80       625\n",
            "            macro avg       0.80      0.80      0.80       625\n",
            "         weighted avg       0.80      0.80      0.80       625\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(test_dataset.target,predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "champion-beijing",
      "metadata": {
        "id": "champion-beijing"
      },
      "source": [
        "\n",
        "\n",
        "| Model | Accuracy | macro F1 |\n",
        "| --- | ----------- | ----|\n",
        "| Naive Bayes | 0.62 | 0.61|\n",
        "| Decision Tree | 0.50 | 0.50 |\n",
        "| Linear SVM | 0.69 | 0.68 |\n",
        "| Non-linear SVM| 0.67 | 0.67 |\n",
        "| BiLSTM| 0.72 | 0.71 |\n",
        "|RoBERTa| 0.80 | 0.80 |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vital-trustee",
      "metadata": {
        "id": "vital-trustee"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}